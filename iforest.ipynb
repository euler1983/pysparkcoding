{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 安装maven, 参考：https://blog.csdn.net/qq_29695701/article/details/90705181\n",
    "\n",
    "2. 安装pyspark==2.4.0\n",
    "\n",
    "3. 按照github: https://github.com/titicaca/spark-iforest\n",
    "里提示的两个步骤进行操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jdk/jdk1.8.0_191'\n",
    "os.environ['SPARK_HOME'] = '/usr/lib/spark/spark-2.4.4-bin-hadoop2.7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import tempfile\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.master(\"local[*]\") \\\n",
    "        .appName(\"IForestExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([7.0, 9.0]),),\n",
    "        (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n",
    "\n",
    "# NOTE: features need to be dense vectors for the model input\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| features|\n",
      "+---------+\n",
      "|[0.0,0.0]|\n",
      "|[7.0,9.0]|\n",
      "|[9.0,8.0]|\n",
      "|[8.0,9.0]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark_iforest.ml.iforest import *\n",
    "\n",
    "# Init an IForest Object\n",
    "iforest = IForest(contamination=0.3, maxDepth=2)\n",
    "\n",
    "# Fit on a given data frame\n",
    "model = iforest.fit(df)\n",
    "\n",
    "# Check if the model has summary or not, the newly trained model has the summary info\n",
    "model.hasSummary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+\n",
      "| features|       anomalyScore|prediction|\n",
      "+---------+-------------------+----------+\n",
      "|[0.0,0.0]| 0.6575333612057278|       1.0|\n",
      "|[7.0,9.0]| 0.3598940106267502|       0.0|\n",
      "|[9.0,8.0]|0.44716436916437347|       0.0|\n",
      "|[8.0,9.0]| 0.3351858475368414|       0.0|\n",
      "+---------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show model summary\n",
    "summary = model.summary\n",
    "\n",
    "# Show the number of anomalies\n",
    "summary.numAnomalies\n",
    "\n",
    "# Predict for a new data frame based on the fitted model\n",
    "transformed = model.transform(df)\n",
    "\n",
    "# Collect spark data frame into local df\n",
    "rows = transformed.collect()\n",
    "\n",
    "temp_path = tempfile.mkdtemp()\n",
    "iforest_path = temp_path + \"/iforest\"\n",
    "\n",
    "# Save the iforest estimator into the path\n",
    "iforest.save(iforest_path)\n",
    "\n",
    "# Load iforest estimator from a path\n",
    "loaded_iforest = IForest.load(iforest_path)\n",
    "\n",
    "model_path = temp_path + \"/iforest_model\"\n",
    "\n",
    "# Save the fitted model into the model path\n",
    "model.save(model_path)\n",
    "\n",
    "# Load a fitted model from a model path\n",
    "loaded_model = IForestModel.load(model_path)\n",
    "\n",
    "# The loaded model has no summary info\n",
    "loaded_model.hasSummary\n",
    "\n",
    "# Use the loaded model to predict a new data frame\n",
    "loaded_model.transform(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
